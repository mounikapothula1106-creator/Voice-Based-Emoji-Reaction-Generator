///VOICE EMOTION DETECTOR///

1. Title
Project Name: VOICE-BASED EMOJI REACTION GENERATOR 

2. Objective
The main objective of this project is to identify human emotions from spoken voice input.
It records the userâ€™s speech, converts it to text, analyzes the text sentiment, and displays a suitable emoji representing the detected emotion.

3. Hardware & Software Requirements
Hardware: Microphone, Computer/Laptop
Software:
-> Python 3
-> VS Code IDE
-> Libraries: speech_recognition, sounddevice, textblob, emoji, numpy
Internet connection (for Google Speech Recognition API)

4. Algorithm / Flow of the Project
-> Record the userâ€™s voice using the microphone.
-> Convert the audio input into text using Google Speech Recognition API.
-> Analyze the text using keyword matching and sentiment analysis.
-> Detect and classify emotion (happy, sad, angry, etc.).
-> Display the emotion with a corresponding emoji.

5. Source Code
â€œThe project consists of two Python files 
voice_emoji.py (main file handling recording, recognition, and emotion detection) and 
record.py (helper file for emotion mapping and emoji output).â€

-> File 1: voice_emoji.py

# voice_emoji.py
import sounddevice as sd
import numpy as np
import speech_recognition as sr
from textblob import TextBlob
import sys

# ğŸ§ Settings
SAMPLE_RATE = 16000   # 16 kHz audio
DURATION = 4          # record duration in seconds
SAMPLE_WIDTH = 2      # bytes per sample for int16


# ğŸ’¬ Emotion detection using keywords + sentiment
def detect_emotion(text):
    text_lower = text.lower()

    # Keyword-based detection
    if any(word in text_lower for word in ["angry", "mad", "furious", "annoyed"]):
        return "ğŸ˜¡", "Angry", -0.7
    elif any(word in text_lower for word in ["love", "awesome", "amazing", "beautiful", "great"]):
        return "ğŸ˜", "Loved", 0.6
    elif any(word in text_lower for word in ["tired", "sleepy", "exhausted", "lazy"]):
        return "ğŸ˜ª", "Tired", -0.1
    elif any(word in text_lower for word in ["shocked", "surprised", "wow", "omg", "what"]):
        return "ğŸ˜²", "Surprised", 0.2
    elif any(word in text_lower for word in ["scared", "afraid", "fear", "terrified"]):
        return "ğŸ˜¨", "Scared", -0.3
    elif any(word in text_lower for word in ["bored", "meh", "nothing", "idk"]):
        return "ğŸ˜", "Bored", 0
    elif any(word in text_lower for word in ["cry", "sad", "upset", "depressed", "disappointed"]):
        return "ğŸ˜”", "Sad", -0.6
    elif any(word in text_lower for word in ["happy", "glad", "joy", "cheerful", "smile"]):
        return "ğŸ˜Š", "Happy", 0.6
    
    # Fallback sentiment analysis
    blob = TextBlob(text)
    polarity = blob.sentiment.polarity
    if polarity > 0.35:
        return "ğŸ˜", "Positive", polarity
    elif polarity < -0.35:
        return "â˜¹ï¸", "Negative", polarity
    else:
        return "ğŸ˜", "Neutral", polarity


# ğŸ™ Record voice using sounddevice
def record_raw(duration=DURATION, fs=SAMPLE_RATE):
    print(f"ğŸ¤ Recording for {duration} seconds â€” speak clearly now...")
    try:
        recording = sd.rec(int(duration * fs), samplerate=fs, channels=1, dtype='int16')
        sd.wait()
        return recording
    except Exception as e:
        print("Recording failed:", e)
        sys.exit(1)


# ğŸ”Š Convert NumPy array â†’ bytes for recognizer
def audiobytes_from_numpy(np_array):
    arr = np_array
    if arr.ndim > 1:
        arr = arr.reshape(-1)
    return arr.tobytes()


# ğŸ§  Speech recognition via Google API
def recognize_from_bytes(audio_bytes, fs=SAMPLE_RATE, sample_width=SAMPLE_WIDTH):
    recognizer = sr.Recognizer()
    audio_data = sr.AudioData(audio_bytes, fs, sample_width)
    try:
        text = recognizer.recognize_google(audio_data)
        return text
    except sr.UnknownValueError:
        return None
    except sr.RequestError as e:
        print("âš ï¸ Speech recognition failed (check internet):", e)
        return None


# ğŸš€ Main logic
def main():
    print("Press Enter to start recording...\n")
    input()
    raw = record_raw()
    audio_bytes = audiobytes_from_numpy(raw)
    
    print("ğŸ§ Processing... please wait...")
    text = recognize_from_bytes(audio_bytes)

    if text is None:
        print("âŒ Could not recognize speech. Try again!")
        return

    emo_char, label, polarity = detect_emotion(text)
    print(f"â¤  {text} {emo_char}")


# ğŸ Run
if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\nExited by user.")

-> File 2: record.py

import sounddevice as sd
import soundfile as sf
import sys
SAMPLE_RATE = 16000  # 16kHz is enough for speech
CHANNELS = 1
DURATION = 3
OUT_FILE = "output.wav"

def record(duration=DURATION, sr=SAMPLE_RATE, channels=CHANNELS, filename=OUT_FILE):
    print(f"Recording for {duration} seconds... Speak now.")
    try:
        recording = sd.rec(int(duration * sr), samplerate=sr, channels=channels, dtype='int16')
        sd.wait()  # wait until recording completes
        sf.write(filename, recording, sr, subtype='PCM_16')
        print(f"Saved recording to '{filename}' successfully.")
    except Exception as e:
        print("Recording failed:", e)
        sys.exit(1)

if __name__ == "__main__":
    record()

6. Output
Example:

Press Enter to start recording...

ğŸ¤ Recording for 4 seconds â€” speak clearly now...
ğŸ§ Processing... please wait...
â¤ I'm very happy ğŸ˜Š

7. Conclusion
This project successfully demonstrates how machine learning and natural language processing can be used to detect emotions from voice input. 
It can be extended for real-time humanâ€“computer interaction or virtual assistants.

8. References
-> Python Documentation
-> SpeechRecognition Library
-> TextBlob Sentiment Analysis
-> Stack Overflow & GitHub Discussions
